{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession,HiveContext\n",
    "from pyspark.sql.functions import explode,col,to_timestamp,udf,unix_timestamp,current_timestamp,from_unixtime,to_date\n",
    "import shutil # util to copy from incremental folder to working folder\n",
    "from datetime import date,datetime\n",
    "from pyspark.sql.types import IntegerType,StringType,DateType,TimestampType \n",
    "import subprocess\n",
    "from pyspark import SparkContext, HiveContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime as dt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e62975ae-1e6a-4b40-ad2b-4e9991745c7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Configurations and Constents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5762ea75-8346-4215-9a18-d0f4a151d5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#########  Reading from local #####################\n",
    "# incremental_directory = \"/Users/jpedapally/Documents/project/intute/source/incremental/\"\n",
    "# historic_directory    = \"/Users/jpedapally/Documents/project/intute/source/historic/\"\n",
    "# archive_directory     = \"/Users/jpedapally/Documents/project/intute/source/archive/\"\n",
    "# error_directory       = \"/Users/jpedapally/Documents/project/intute/source/error/\"\n",
    "# working_directory     = \"/Users/jpedapally/Documents/project/intute/source/working/\"\n",
    "# output_directory      = \"/Users/jpedapally/Documents/project/intute/source/output/\"\n",
    "# temp_directory        = \"/Users/jpedapally/Documents/project/intute/source/temp/\"\n",
    "\n",
    "# #########  Reading from HDFS #####################\n",
    "incremental_directory = \"hdfs://localhost:9000/source/incremental/\"\n",
    "historic_directory    = \"hdfs://localhost:9000/source/historic/\"\n",
    "archive_directory     = \"hdfs://localhost:9000/source/archive/\"\n",
    "error_directory       = \"hdfs://localhost:9000/source/error/\"\n",
    "working_directory     = \"hdfs://localhost:9000/source/working/\"\n",
    "output_directory      = \"hdfs://localhost:9000/source/output/\"\n",
    "temp_directory        = \"hdfs://localhost:9000/source/temp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e349b538-6730-4dac-b476-ecaff3c25477",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65a16d56-f892-434e-a379-f412806b3d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    ".builder.master(\"localhost:9000\")\\\n",
    ".appName(\"jdbc data sources\")\\\n",
    ".config(\"spark.driver.bindAddress\",\"127.0.0.1\")\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24d340b8-66ae-4a39-a787-6414400f8fe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# # User functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c9abd7f6-cf78-4fdb-9d0e-4418aa3f3b58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_df(df):\n",
    "    df.printSchema()\n",
    "    df.show(5,False)\n",
    "\n",
    "def read_directory(directory):\n",
    "    ########## Reading all files in directory ########## \n",
    "    print (\"=================================================================================================\")\n",
    "    print (f\"Started reading '{directory}' files in working_directory {datetime.now()}\")\n",
    "    print (\"=================================================================================================\")\n",
    "    df = spark.read.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"inferschems\",\"True\").format(\"json\").load(f\"{directory}*.json\")\n",
    "    print (f\"Read file {directory}\")  \n",
    "    print (\"=================================================================================================\")\n",
    "    print (f\"Total we read '{directory}' files in working_directory {datetime.now()}\")\n",
    "    print (\"=================================================================================================\") \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_dups(df):\n",
    "    return df.dropDuplicates([\"Agentid\",\"CallStartTimestamp\",\"Channel\",\"ContactId\"]) \n",
    "    \n",
    "\n",
    "    \n",
    "def remove_missing_data(df):\n",
    "    return df.where(\"Agentid IS NOT NULL AND CallStartTimestamp IS NOT NULL AND CallEndTimestamp IS NOT NULL\")\n",
    "    \n",
    "    \n",
    "def clean_df(df):\n",
    "    df_exp = df.withColumn(\"AgentCallbackMessage\", col(\"Attributes.AgentCallbackMessage\"))\\\n",
    "    .withColumn(\"AgentHoldLoopDuration\", col(\"Attributes.AgentHoldLoopDuration\"))\\\n",
    "    .withColumn(\"CallRecordingEnabled\", col(\"Attributes.CallRecordingEnabled\"))\\\n",
    "    .withColumn(\"OfferQualityFeedback\", col(\"Attributes.OfferQualityFeedback\"))\\\n",
    "    .withColumn(\"QueueDuration\", col(\"Attributes.QueueDuration\"))\\\n",
    "    .withColumn(\"QueueName\", col(\"Attributes.QueueName\"))\\\n",
    "    .withColumn(\"QueueOverrideEnabled\", col(\"Attributes.QueueOverrideEnabled\"))\\\n",
    "    .withColumn(\"QueueType\", col(\"Attributes.QueueType\"))\\\n",
    "    .withColumn(\"Rating\", col(\"Attributes.Rating\")) \n",
    "    return df_exp\n",
    "\n",
    "\n",
    "udf\n",
    "def diff_time_in_sec(start_time,end_time):\n",
    "    diff  = end_time - start_time \n",
    "    return diff.seconds\n",
    "\n",
    "udf_diff_time_in_sec = udf(diff_time_in_sec,IntegerType())\n",
    "\n",
    "def run_cmd(args_list):\n",
    "        \"\"\"\n",
    "        run linux commands\n",
    "        \"\"\"\n",
    "        # import subprocess\n",
    "        print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "        proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        s_output, s_err = proc.communicate()\n",
    "        s_return =  proc.returncode\n",
    "        return s_return, s_output, s_err \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b1c265f-0fea-475a-9501-af278affd5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Read all files in working directory\n",
    "# Check if we have multiple files and move them to working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "Started reading 'hdfs://localhost:9000/source/incremental/' files in working_directory 2022-01-11 22:50:26.302340\n",
      "=================================================================================================\n",
      "Read file hdfs://localhost:9000/source/incremental/\n",
      "=================================================================================================\n",
      "Total we read 'hdfs://localhost:9000/source/incremental/' files in working_directory 2022-01-11 22:50:26.503413\n",
      "=================================================================================================\n",
      "root\n",
      " |-- Agentid: string (nullable = true)\n",
      " |-- Attributes: struct (nullable = true)\n",
      " |    |-- AgentCallbackMessage: string (nullable = true)\n",
      " |    |-- AgentHoldLoopDuration: string (nullable = true)\n",
      " |    |-- CallRecordingEnabled: string (nullable = true)\n",
      " |    |-- OfferQualityFeedback: string (nullable = true)\n",
      " |    |-- QueueDuration: string (nullable = true)\n",
      " |    |-- QueueName: string (nullable = true)\n",
      " |    |-- QueueOverrideEnabled: string (nullable = true)\n",
      " |    |-- QueueType: string (nullable = true)\n",
      " |    |-- Rating: string (nullable = true)\n",
      " |-- CallEndTimestamp: timestamp (nullable = true)\n",
      " |-- CallStartTimestamp: timestamp (nullable = true)\n",
      " |-- Channel: string (nullable = true)\n",
      " |-- ContactId: string (nullable = true)\n",
      " |-- InitialContactId: string (nullable = true)\n",
      " |-- InitiationMethod: string (nullable = true)\n",
      " |-- NextContactId: string (nullable = true)\n",
      " |-- PreviousContactId: string (nullable = true)\n",
      " |-- AgentCallbackMessage: string (nullable = true)\n",
      " |-- AgentHoldLoopDuration: string (nullable = true)\n",
      " |-- CallRecordingEnabled: string (nullable = true)\n",
      " |-- OfferQualityFeedback: string (nullable = true)\n",
      " |-- QueueDuration: string (nullable = true)\n",
      " |-- QueueName: string (nullable = true)\n",
      " |-- QueueOverrideEnabled: string (nullable = true)\n",
      " |-- QueueType: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- recordAddedOn: timestamp (nullable = false)\n",
      " |-- recordUpdatedOn: timestamp (nullable = false)\n",
      " |-- partitionByDate: date (nullable = true)\n",
      " |-- callDuration: integer (nullable = true)\n",
      "\n",
      "+-------------+------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+-------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+-----------------------+-----------------------+---------------+------------+\n",
      "|Agentid      |Attributes                                |CallEndTimestamp   |CallStartTimestamp |Channel|ContactId                            |InitialContactId|InitiationMethod|NextContactId                        |PreviousContactId|AgentCallbackMessage|AgentHoldLoopDuration|CallRecordingEnabled|OfferQualityFeedback|QueueDuration|QueueName|QueueOverrideEnabled|QueueType|Rating|recordAddedOn          |recordUpdatedOn        |partitionByDate|callDuration|\n",
      "+-------------+------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+-------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+-----------------------+-----------------------+---------------+------------+\n",
      "|6k5cfa8a-24b9|[, 60, true, false, 0, s-queue,, , ]      |2019-12-31 08:43:09|2019-12-31 08:38:11|CHAT   |665cfa8a-24b9-f4346-8cc0-a690a777e42f|null            |OUTBOUND        |665cfa8a-25b9-4346-8cc0-a99d0a777e42f|null             |                    |60                   |true                |false               |0            |s-queue  |null                |         |      |2022-01-11 22:50:26.632|2022-01-11 22:50:26.632|2019-12-31     |298         |\n",
      "|6h5cfa8a-24b9|[, 60, true, false, 0, g-queue,, , ]      |2019-12-31 14:43:09|2019-12-31 14:38:11|CHAT   |665cfa8a-25b9-f4346-8cc0-a990a777e42f|null            |OUTBOUND        |                                     |null             |                    |60                   |true                |false               |0            |g-queue  |null                |         |      |2022-01-11 22:50:26.632|2022-01-11 22:50:26.632|2019-12-31     |298         |\n",
      "|645cfa8a-24b9|[, 60, true, false, 0, l-queue, false, , ]|2019-12-30 19:28:34|2019-12-30 19:23:34|VOICE  |665cfa8a-25b9-4346-8cc0-a99d0a777e42f|null            |OUTBOUND        |                                     |null             |                    |60                   |true                |false               |0            |l-queue  |false               |         |      |2022-01-11 22:50:26.632|2022-01-11 22:50:26.632|2019-12-30     |300         |\n",
      "|4gyia8a-24b9 |[, 60, true, false, 0, b-queue, false, , ]|2019-12-30 19:51:12|2019-12-30 19:48:11|CHAT   |645cfa8a-24nb9-4346-8cc0-a990a777e42f|null            |INBOUND         |                                     |null             |                    |60                   |true                |false               |0            |b-queue  |false               |         |      |2022-01-11 22:50:26.632|2022-01-11 22:50:26.632|2019-12-30     |181         |\n",
      "|4gyra8a-24b9 |[, 60, true, false, 0, n-queue, false, , ]|2019-12-30 19:50:11|2019-12-30 19:38:09|VOICE  |965cfa8a-24b9-43m46-8cc0-a990a777e42f|null            |INBOUND         |                                     |null             |                    |60                   |true                |false               |0            |n-queue  |false               |         |      |2022-01-11 22:50:26.632|2022-01-11 22:50:26.632|2019-12-30     |722         |\n",
      "+-------------+------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+-------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+-----------------------+-----------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ################ move incremental files to working_directory ########################\n",
    "#     (ret, out, err)= run_cmd(['hdfs', 'dfs', '-mv', f'{incremental_directory}', f'{working_directory}'])\n",
    "\n",
    "    \n",
    "    \n",
    "    ################ maintain the tract of files_names read ########################\n",
    "    \n",
    "    #### To check what all files are processed  #### \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    number_files_working_directory = 3\n",
    "    ################ After moving to working dir cleaning the files ########################\n",
    "    if number_files_working_directory > 0:\n",
    "        df_incr = read_directory(incremental_directory)  #### Read hdfs files ####\n",
    "        df_incr_cleaned = clean_df(df_incr)  \n",
    "        df_incr_no_dups=remove_dups(df_incr_cleaned)\n",
    "        df_incr_intr = remove_missing_data(df_incr_no_dups)\n",
    "        df_incr_f = df_incr_intr.withColumn(\"recordAddedOn\",current_timestamp())\\\n",
    "        .withColumn(\"recordAddedOn\",current_timestamp())\\\n",
    "        .withColumn(\"recordUpdatedOn\",current_timestamp())\\\n",
    "        .withColumn(\"CallStartTimestamp\",to_timestamp(\"CallStartTimestamp\"))\\\n",
    "        .withColumn(\"CallEndTimestamp\",to_timestamp(\"CallEndTimestamp\"))\\\n",
    "        .withColumn(\"partitionByDate\",to_date(\"CallStartTimestamp\"))\\\n",
    "        .where(\"Agentid IS NOT NULL AND CallStartTimestamp IS NOT NULL AND CallEndTimestamp IS NOT NULL AND Channel IS NOT NULL\")\n",
    "\n",
    "\n",
    "        df_incr_f0 = df_incr_f.withColumn(\"callDuration\",udf_diff_time_in_sec((col(\"CallStartTimestamp\")),(col(\"CallEndTimestamp\"))))\n",
    "        df_incr_f1 = df_incr_f0.coalesce(1)\n",
    "        \n",
    "        df_incr_final= df_incr_f1\n",
    "        display_df(df_incr_final)\n",
    "        \n",
    "        df_incr_final.coalesce(1).write.mode(\"overwrite\").parquet(f'{temp_directory}')\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        print (\"No files to process\")\n",
    "        exit(0)\n",
    "except FileNotFoundError as e:\n",
    "    print (\"No files to process\")\n",
    "    print (repr(e))\n",
    "except Except as e:\n",
    "    print (repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85960761-43af-4821-9863-e718010fe95c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Merg incr and Hist files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ https://dwgeek.com/sql-merge-operation-using-pyspark-upsert-example.html/\n",
    "df_incr_final.createOrReplaceTempView(\"incr_table\")\n",
    "df_hist_final.createOrReplaceTempView(\"hist_table_full\")\n",
    "\n",
    "\n",
    "################ historic df without incremental dates ########################\n",
    "df_hist_table_without_inc_dates = spark.sql(\"select * from hist_table_full where partitionByDate not in (select distinct date(CallStartTimestamp) from incr_table) \")\n",
    "\n",
    "################ historic df with incremental dates ########################\n",
    "df_hist_table_with_inc_dates = spark.sql(\"select * from hist_table_full where partitionByDate in (select distinct date(CallStartTimestamp) from incr_table) \")\n",
    "\n",
    "\n",
    "################ historic df with incremental dates UNION with incremental data ########################\n",
    "\n",
    "df_hist_union_incr = df_hist_table_with_inc_dates.unionByName(df_incr_final)\n",
    "\n",
    "df_merge = df_hist_union_incr.withColumn(\"_row_number\", \\\n",
    "                                         row_number()\\\n",
    "                                         .over(Window\\\n",
    "                                               .partitionBy \\\n",
    "                                               (df_hist_union_incr['Agentid'],\\\n",
    "                                                df_hist_union_incr['CallStartTimestamp'],\\\n",
    "                                                df_hist_union_incr['Channel'])\\\n",
    "                                               .orderBy('recordUpdatedOn')))\n",
    "# df_merge.printSchema()\n",
    "# df_merge.where(\"_row_number > 1\").show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the dataframe to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One time load of historinc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================\n",
      "Started reading 'hdfs://localhost:9000/source/error/' files in working_directory 2022-01-10 01:04:03.898072\n",
      "=================================================================================================\n",
      "Read file hdfs://localhost:9000/source/error/\n",
      "=================================================================================================\n",
      "Total we read 'hdfs://localhost:9000/source/error/' files in working_directory 2022-01-10 01:04:03.987278\n",
      "=================================================================================================\n",
      "root\n",
      " |-- Agentid: string (nullable = true)\n",
      " |-- Attributes: struct (nullable = true)\n",
      " |    |-- AgentCallbackMessage: string (nullable = true)\n",
      " |    |-- AgentHoldLoopDuration: string (nullable = true)\n",
      " |    |-- CallRecordingEnabled: string (nullable = true)\n",
      " |    |-- OfferQualityFeedback: string (nullable = true)\n",
      " |    |-- QueueDuration: string (nullable = true)\n",
      " |    |-- QueueName: string (nullable = true)\n",
      " |    |-- QueueOverrideEnabled: string (nullable = true)\n",
      " |    |-- QueueType: string (nullable = true)\n",
      " |    |-- Rating: string (nullable = true)\n",
      " |-- CallEndTimestamp: timestamp (nullable = true)\n",
      " |-- CallStartTimestamp: timestamp (nullable = true)\n",
      " |-- Channel: string (nullable = true)\n",
      " |-- ContactId: string (nullable = true)\n",
      " |-- InitialContactId: string (nullable = true)\n",
      " |-- InitiationMethod: string (nullable = true)\n",
      " |-- NextContactId: string (nullable = true)\n",
      " |-- PreviousContactId: string (nullable = true)\n",
      " |-- AgentCallbackMessage: string (nullable = true)\n",
      " |-- AgentHoldLoopDuration: string (nullable = true)\n",
      " |-- CallRecordingEnabled: string (nullable = true)\n",
      " |-- OfferQualityFeedback: string (nullable = true)\n",
      " |-- QueueDuration: string (nullable = true)\n",
      " |-- QueueName: string (nullable = true)\n",
      " |-- QueueOverrideEnabled: string (nullable = true)\n",
      " |-- QueueType: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- recordAddedOn: timestamp (nullable = false)\n",
      " |-- recordUpdatedOn: timestamp (nullable = false)\n",
      " |-- partitionByDate: date (nullable = true)\n",
      " |-- callDuration: integer (nullable = true)\n",
      "\n",
      "+-------------+-------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+----------------------+----------------------+---------------+------------+\n",
      "|Agentid      |Attributes                                 |CallEndTimestamp   |CallStartTimestamp |Channel|ContactId                            |InitialContactId|InitiationMethod|NextContactId                       |PreviousContactId|AgentCallbackMessage|AgentHoldLoopDuration|CallRecordingEnabled|OfferQualityFeedback|QueueDuration|QueueName|QueueOverrideEnabled|QueueType|Rating|recordAddedOn         |recordUpdatedOn       |partitionByDate|callDuration|\n",
      "+-------------+-------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+----------------------+----------------------+---------------+------------+\n",
      "|665cfa8a-24b9|[, 60, true, false, 0, p-queue,, , ]       |2019-12-31 03:14:51|2019-12-31 03:10:56|CHAT   |665cfa8a-54b9-4346-8cc0-a990a7577e42f|null            |OUTBOUND        |                                    |null             |                    |60                   |true                |false               |0            |p-queue  |null                |         |      |2022-01-10 01:04:04.06|2022-01-10 01:04:04.06|2019-12-31     |235         |\n",
      "|4gyua8a-24b9 |[, 60, true, false, 0, p-queue, false, , ] |2019-10-12 16:54:20|2019-10-12 16:44:20|VOICE  |665cfa8a-24b9-4346-8cc2-a890a777e42f |null            |INBOUND         |                                    |null             |                    |60                   |true                |false               |0            |p-queue  |false               |         |      |2022-01-10 01:04:04.06|2022-01-10 01:04:04.06|2019-10-12     |600         |\n",
      "|4gyua8a-24b9 |[, 60, true, false, 0, p-queue, false, , ] |2019-12-31 15:54:30|2019-12-31 15:44:30|VOICE  |665cfa8a-24b9-4349-8cc0-a990a777e42f |null            |INBOUND         |                                    |null             |                    |60                   |true                |false               |0            |p-queue  |false               |         |      |2022-01-10 01:04:04.06|2022-01-10 01:04:04.06|2019-12-31     |600         |\n",
      "|9656ua8a-24b9|[, 60, true, false, 15, s-queue, false, , ]|2019-12-31 02:29:56|2019-12-31 02:22:12|CHAT   |665cfa8a-24b9-4346-8uc0-a990a777e42f |null            |INBOUND         |665cfa8a-24b9-4346-8cc0-a990a774e42f|null             |                    |60                   |true                |false               |15           |s-queue  |false               |         |      |2022-01-10 01:04:04.06|2022-01-10 01:04:04.06|2019-12-31     |464         |\n",
      "|7546u6tg-7huj|[, 60, true, false, 0, p-queue, false, , ] |2019-10-13 20:28:18|2019-10-13 20:23:18|CHAT   |665cfga8a-24b9-4346-8cc0-a990a777e42f|null            |INBOUND         |                                    |null             |                    |60                   |true                |false               |0            |p-queue  |false               |         |      |2022-01-10 01:04:04.06|2022-01-10 01:04:04.06|2019-10-13     |300         |\n",
      "+-------------+-------------------------------------------+-------------------+-------------------+-------+-------------------------------------+----------------+----------------+------------------------------------+-----------------+--------------------+---------------------+--------------------+--------------------+-------------+---------+--------------------+---------+------+----------------------+----------------------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ################ historic df with incremental dates UNION with incremental data ########################\n",
    "\n",
    "# # bin/hdfs dfs -ls /source/error/\n",
    "# # bin/hdfs dfs -rm -r /source/error/\n",
    "# # bin/hdfs dfs -put /Users/jpedapally/Documents/project/intute/source/historic/*.json /source/error/\n",
    "# # bin/hdfs dfs -ls /source/error/\n",
    "# # bin/hdfs dfs -get hdfs://localhost:9000/source/historic/ /Users/jpedapally/Documents/project/intute/source/historic/\n",
    "\n",
    "\n",
    "# # https://stackoverflow.com/questions/53329250/difference-between-dates-in-pyspark-sql\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_hist = read_directory(error_directory)  #### Read hdfs files ####\n",
    "# df_hist_cleaned = clean_df(df_hist)  \n",
    "# df_hist_no_dups=remove_dups(df_hist_cleaned)\n",
    "# df_hist_intr = remove_missing_data(df_hist_no_dups)\n",
    "# df_hist_f = df_hist_intr.withColumn(\"recordAddedOn\",current_timestamp())\\\n",
    "# .withColumn(\"recordAddedOn\",current_timestamp())\\\n",
    "# .withColumn(\"recordUpdatedOn\",current_timestamp())\\\n",
    "# .withColumn(\"CallStartTimestamp\",to_timestamp(\"CallStartTimestamp\"))\\\n",
    "# .withColumn(\"CallEndTimestamp\",to_timestamp(\"CallEndTimestamp\"))\\\n",
    "# .withColumn(\"partitionByDate\",to_date(\"CallStartTimestamp\"))\\\n",
    "# .where(\"Agentid IS NOT NULL AND CallStartTimestamp IS NOT NULL AND CallEndTimestamp IS NOT NULL AND Channel IS NOT NULL\")\n",
    " \n",
    "\n",
    "# df_hist_final = df_hist_f.withColumn(\"callDuration\",udf_diff_time_in_sec((col(\"CallStartTimestamp\")),(col(\"CallEndTimestamp\"))))\n",
    " \n",
    "# display_df(df_hist_final) \n",
    "\n",
    "# df_hist_final.write.mode(\"overwrite\").partitionBy(\"partitionByDate\").parquet(f'{historic_directory}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Big Query Analysis ################\n",
    "# -----------------------------------------------------------------------\n",
    "# -- Please write SQL queries to find answers of the following business questions from call_logs table\n",
    "# -- a. What is the average length in seconds of all calls (i.e. all channels)\n",
    "# -- received by each agent between Oct-15-2019 and Oct-30-2019?\n",
    "# -----------------------------------------------------------------------\n",
    "# select AVG(callDuration)  from `projectn-281322.calldata.call_data_historic`\n",
    "# where date(CallStartTimestamp) between '2019-10-15' AND '2019-10-30'\n",
    "# AND InitiationMethod = 'INBOUND'; \n",
    "# -----------------------------------------------------------------------\n",
    "# -- b. Get total number of calls served by each agent per channel group.\n",
    "# -----------------------------------------------------------------------\n",
    "# select Agentid,Channel,InitiationMethod, count(concat(CallEndTimestamp,\tCallStartTimestamp)) number_of_call  \n",
    "# from `projectn-281322.calldata.call_data_historic`\n",
    "# group by 1,2,3 order by 1,2,3;\n",
    "# -----------------------------------------------------------------------\n",
    "# -- c. Show top 5 agents based on number of calls per month.\n",
    "# -----------------------------------------------------------------------\n",
    "# select extract(month from CallEndTimestamp) month,Agentid,Channel,InitiationMethod, count(concat(CallEndTimestamp,\tCallStartTimestamp))  \n",
    "# from `projectn-281322.calldata.call_data_historic`\n",
    "# group by 1,2,3,4 order by 1,2,3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a14399a4-c000-40dc-82e9-21962c633c67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# pseudo code load redshift from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a2f763e-dfe8-49c9-ad32-525c547c141d",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-6f9d45d48f06>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-6f9d45d48f06>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    export CLASSPATH=$PWD/RedshiftJDBC42-1.1.17.1017.jar\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# pseudo code load redshift from s3\n",
    "\n",
    "# https://dwgeek.com/how-to-export-spark-dataframe-to-redshift-table.html/\n",
    "\n",
    "export CLASSPATH=$PWD/RedshiftJDBC42-1.1.17.1017.jar\n",
    "\n",
    "pyspark --conf spark.executor.extraClassPath=/path/RedshiftJDBC42-1.1.17.1017.jar --driver-class-path /path/RedshiftJDBC42-1.1.17.1017.jar --jars /path/RedshiftJDBC42-1.1.17.1017.jar\n",
    "\n",
    "testDf.write.format('jdbc').options(\n",
    "      url='jdbc:redshift://testredshift.us-east-1.redshift.amazonaws.com:5439/dev',\t  \n",
    "      driver='com.amazon.redshift.jdbc42.Driver',\n",
    "      dbtable='public.df_load_test',\n",
    "      user='redshiftuser',\n",
    "      password='Password').mode('append').save() \n",
    "\n",
    "testDf.write.format('jdbc').options(\n",
    "      url='jdbc:redshift://testredshift.us-east-1.redshift.amazonaws.com:5439/dev',\t  \n",
    "      driver='com.amazon.redshift.jdbc42.Driver',\n",
    "      dbtable='public.df_load_test',\n",
    "      user='redshiftuser',\n",
    "      password='Password').mode('overwrite').save() \n",
    "\n",
    "\n",
    "\n",
    "# Create partition table , https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html\n",
    "create external table spectrum.sales_part(\n",
    "salesid integer,\n",
    "listid integer,\n",
    "sellerid integer,\n",
    "buyerid integer,\n",
    "eventid integer,\n",
    "dateid smallint,\n",
    "qtysold smallint,\n",
    "pricepaid decimal(8,2),\n",
    "commission decimal(8,2),\n",
    "saletime timestamp)\n",
    "partitioned by (salesmonth char(10), event integer)\n",
    "row format delimited\n",
    "fields terminated by '|'\n",
    "stored as parquet \n",
    "location 's3://awssampledbuswest2/tickit/spectrum/sales_partition/'\n",
    "table properties ('numRows'='172000');\n",
    "\n",
    "# Drop partitions\n",
    "alter table spectrum.sales_part\n",
    "drop partition(saledate='2008-01-01');\n",
    "\n",
    "# Add partitions\n",
    "alter table spectrum.sales_part add\n",
    "partition(saledate='2008-01') \n",
    "location 's3://awssampledbuswest2/tickit/spectrum/sales_partition/saledate=2008-01/'\n",
    "\n",
    "partition(saledate='2008-02') \n",
    "location 's3://awssampledbuswest2/tickit/spectrum/sales_partition/saledate=2008-02/'\n",
    "\n",
    "partition(saledate='2008-03') \n",
    "location 's3://awssampledbuswest2/tickit/spectrum/sales_partition/saledate=2008-03/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful infromation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/ClusterSetup.html\n",
    "# https://techblost.com/how-to-install-hive-on-mac-with-homebrew/\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "434b3e68-e65d-4c98-806b-088ee412c467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files to process\n",
      "FileNotFoundError(2, 'No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "############## Below commands if you are running from local #########################\n",
    "# try:\n",
    "#     ################ Move incr to working (mv incr to working) ########################\n",
    "#     number_files_incremental_directory = 0\n",
    "#     incremental_directory_files = []\n",
    "#     incremental_directory_files = [path for path in os.listdir(incremental_directory) if os.path.isfile(os.path.join(incremental_directory, path))]\n",
    "#     number_files_incremental_directory = len(incremental_directory_files)\n",
    "\n",
    "    \n",
    "#     print (f\"We have {incremental_directory_files}  '{number_files_incremental_directory}' files in {incremental_directory} \")\n",
    "    \n",
    "#     if number_files_incremental_directory > 0 :\n",
    "#         allfiles = os.listdir(incremental_directory)\n",
    "#         for f in allfiles:\n",
    "#             shutil.move(incremental_directory + f, working_directory + f)\n",
    "#         print (f\"Moved '{number_files_incremental_directory}' files from incremental_directory to working_directory\")\n",
    "    \n",
    "#     number_files_working_directory = 0\n",
    "#     working_directory_files = []\n",
    "#     working_directory_files = [path for path in os.listdir(working_directory) if os.path.isfile(os.path.join(working_directory, path))]\n",
    "#     number_files_working_directory = len(working_directory_files) \n",
    "            \n",
    "#     print (f\"We have {working_directory_files} '{number_files_working_directory}' files in {incremental_directory} \")    \n",
    "    \n",
    "#     ################ After moving to working dir cleaning the files ########################\n",
    "#     if number_files_working_directory > 0:\n",
    "#         df_incr = read_directory(working_directory)\n",
    "#         df_incr_cleaned = clean_df(df_incr)\n",
    "# #         df_incr_final = df_incr_cleaned.withColumn(\"callDuration\",udf_diff_time_in_sec(df_incr_cleaned[\"CallStartTimestamp\"],df_incr_cleaned[\"CallEndTimestamp\"]).cast(IntegerType()))\n",
    "        \n",
    "        \n",
    "#     else:\n",
    "#         print (\"No files to process\")\n",
    "#         exit(0)\n",
    "# except FileNotFoundError as e:\n",
    "#     print (\"No files to process\")\n",
    "#     print (repr(e))\n",
    "# except Except as e:\n",
    "#     print (repr(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3c1bfdbf-7635-4b7a-8331-c943526b6d1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######## Start Hadoop  #############\n",
    "# $HADOOP_HOME/bin/hdfs --daemon start namenode \n",
    "# http://localhost:9870/dfshealth.html#tab-overview\n",
    "# /usr/local/Cellar/hadoop/3.3.1/libexec\n",
    "\n",
    "######## Hadoop hdfs create directory/folder  #############\n",
    "# bin/hdfs dfs -mkdir /source/incremental/\n",
    "# bin/hdfs dfs -mkdir /source/historic/\n",
    "# bin/hdfs dfs -mkdir /source/archive/\n",
    "# bin/hdfs dfs -mkdir /source/error/\n",
    "# bin/hdfs dfs -mkdir /source/working/\n",
    "# bin/hdfs dfs -mkdir /source/output/\n",
    "\n",
    "######## copy data from local to hdfs  #############\n",
    "# hadoop fs -put /Users/jpedapally/Documents/project/intute/source/working/call_logs_data_202112305750.json /user/hadoop/hadoopfile.csv\n",
    "# bin/hdfs dfs -put /Users/jpedapally/Documents/project/intute/source/working/call_logs_data_202112305750.json /source/working/\n",
    "# bin/hdfs dfs -put /Users/jpedapally/Documents/project/intute/source/historic/*.json /source/error/\n",
    "\n",
    "######## check files exist in HDFS  #############\n",
    "# bin/hdfs dfs -ls /source/\n",
    "\n",
    "\n",
    "######## Stop Hadoop  #############\n",
    "# $HADOOP_HOME/bin/hdfs --daemon stop namenode\n",
    "# $HADOOP_HOME/bin/hdfs --daemon stop datanode\n",
    "# $HADOOP_HOME/sbin/stop-dfs.sh\n",
    "# $HADOOP_HOME/bin/yarn --daemon stop resourcemanager\n",
    "# $HADOOP_HOME/bin/yarn --daemon stop nodemanager\n",
    "# $HADOOP_HOME/sbin/stop-yarn.sh\n",
    "# $HADOOP_HOME/bin/yarn stop proxyserver\n",
    "# $HADOOP_HOME/bin/mapred --daemon stop historyserver\n",
    "\n",
    "######## useful links for inital setup  #############\n",
    "#https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/ClusterSetup.html\n",
    "# https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### To check how many files exists ##########\n",
    "# import subprocess\n",
    "# df = spark.read.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"inferschems\",\"True\").format(\"json\").load(f\"{incremental_directory}*.json\")\n",
    "# df.printSchema()\n",
    "# def run_cmd(args_list):\n",
    "#         \"\"\"\n",
    "#         run linux commands\n",
    "#         \"\"\"\n",
    "#         # import subprocess\n",
    "#         print('Running system command: {0}'.format(' '.join(args_list)))\n",
    "#         proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#         s_output, s_err = proc.communicate()\n",
    "#         s_return =  proc.returncode\n",
    "#         return s_return, s_output, s_err \n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-count', f'{working_directory}'])\n",
    "# lines = out.split()\n",
    "# print (ret)\n",
    "# print(out)\n",
    "# print(err)\n",
    "# print(\"---------------\")\n",
    "# print(lines)\n",
    "\n",
    "# directory = \"/Volumes/GoogleDrive/My Drive/Training/Pyspark/Excersise/intute/source/incremental\"\n",
    "# test_file = [path for path in os.listdir(directory) if os.path.isfile(os.path.join(directory, path))]\n",
    "# print (test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Hive commands ############\n",
    "\n",
    "\n",
    "# show databases;\n",
    "\n",
    "# create database call_data_db;\n",
    "\n",
    "# show databases;\n",
    "\n",
    "# use call_data_db;\n",
    "\n",
    "\n",
    "# CREATE EXTERNAL TABLE call_data_db.call_data_historic_daily \n",
    "# ( \n",
    "# Agentid                          string,          \n",
    "# Attributes                       struct        \n",
    "# <AgentCallbackMessage     :  string,            \n",
    "#  AgentHoldLoopDuration    :  string,             \n",
    "#  CallRecordingEnabled     :  string,             \n",
    "#  OfferQualityFeedback     :  string,             \n",
    "#  QueueDuration            :  string,             \n",
    "#  QueueName                :  string,             \n",
    "#  QueueOverrideEnabled     :  string,             \n",
    "#  QueueType                :  string,             \n",
    "#  Rating                   :  string> ,           \n",
    "# CallEndTimestamp                 timestamp,       \n",
    "# CallStartTimestamp               timestamp,       \n",
    "# Channel                          string,          \n",
    "# ContactId                        string,          \n",
    "# InitialContactId                 string,          \n",
    "# InitiationMethod                 string,          \n",
    "# NextContactId                    string,          \n",
    "# PreviousContactId                string,          \n",
    "# AgentCallbackMessage             string,          \n",
    "# AgentHoldLoopDuration            string,          \n",
    "# CallRecordingEnabled             string,          \n",
    "# OfferQualityFeedback             string,          \n",
    "# QueueDuration                    string,          \n",
    "# QueueName                        string,          \n",
    "# QueueOverrideEnabled             string,          \n",
    "# QueueType                        string,          \n",
    "# Rating                           string,          \n",
    "# callDuration                     integer           \n",
    "# )  PARTITIONED BY (partitionByDate date)\n",
    "# row format delimited fields terminated by ',' \n",
    "# stored as parquet ;\n",
    "\n",
    "\n",
    "# LOAD DATA INPATH 'hdfs://localhost:9000/source/historic/' overwrite into table call_data_db.call_data_historic_daily;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################### Interacting with Hadoop HDFS using Python codes ##################### \n",
    "\n",
    "# # https://community.cloudera.com/t5/Community-Articles/Interacting-with-Hadoop-HDFS-using-Python-codes/ta-p/245163\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run Hadoop ls command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-ls', 'hdfs_file_path'])\n",
    "# lines = out.split('\\n')\n",
    "\n",
    "\n",
    "# Run Hadoop get command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-get', 'hdfs_file_path', 'local_path'])\n",
    "\n",
    "\n",
    "# Run Hadoop put command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-put', 'local_file', 'hdfs_file_path'])\n",
    "\n",
    "\n",
    "# Run Hadoop copyFromLocal command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-copyFromLocal', 'local_file', 'hdfs_file_path'])\n",
    "                            \n",
    "# Run Hadoop copyToLocal command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-copyToLocal', 'hdfs_file_path', 'local_file'])\n",
    "\n",
    "\n",
    "# hdfs dfs -rm -skipTrash /path/to/file/you/want/to/remove/permanently\n",
    "# Run Hadoop remove file command in Python\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-rm', 'hdfs_file_path'])\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-rm', '-skipTrash', 'hdfs_file_path'])\n",
    "\n",
    "\n",
    "# rm -r\n",
    "# HDFS Command to remove the entire directory and all of its content from HDFS.\n",
    "# Usage: hdfs dfs -rm -r <path>\n",
    "    \n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-rm', '-r', 'hdfs_file_path'])\n",
    "# (ret, out, err)= run_cmd(['hdfs', 'dfs', '-rm', '-r', '-skipTrash', 'hdfs_file_path'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if a file exist in HDFS\n",
    "# Usage: hadoop fs -test -[defsz] URI\n",
    "\n",
    "\n",
    "# Options:\n",
    "\n",
    "\n",
    "# -d: f the path is a directory, return 0.\n",
    "# -e: if the path exists, return 0.\n",
    "# -f: if the path is a file, return 0.\n",
    "# -s: if the path is not empty, return 0.\n",
    "# -z: if the file is zero length, return 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark UI description Job,Stage,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/spark/spark-web-ui-understanding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ###### //Transformation\n",
    "# df = spark.read.option(\"inferSchema\",True).option(\"header\",True).csv(path = \"/path/for/the/file\")\n",
    "# //Action\n",
    "# df.count() -->\n",
    "\n",
    "-------------------- \n",
    "Number of Spark Jobs:\n",
    "--------------------\n",
    "The number of Spark jobs is equal to the number of actions in the application and each Spark job should have at least one Stage.\n",
    "In our above application, we have performed 3 Spark jobs (0,1,2)\n",
    "\n",
    "Job 0. read the CSV file.\n",
    "Job 1. Inferschema from the file.\n",
    "Job 2. Count Check\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "Number of Stages:\n",
    "--------------------\n",
    "Each Wide Transformation results in a separate Number of Stages. \n",
    "In our case, Spark job0 and Spark job1 have individual single stages but when \n",
    "it comes to Spark job 3 we can see two stages that are because of the partition of data. \n",
    "Data is partitioned into two files by default.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "craft demo",
   "notebookOrigID": 1483573713852823,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
